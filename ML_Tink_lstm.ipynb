{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Влад\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, roc_auc_score \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "np.random.seed(101)\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>flag</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250977</th>\n",
       "      <td>0</td>\n",
       "      <td>1983354434</td>\n",
       "      <td>Sun May 31 12:59:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Yushimi</td>\n",
       "      <td>@Kardboard yea  ugh. I don't wanna move either...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150064</th>\n",
       "      <td>0</td>\n",
       "      <td>1883616871</td>\n",
       "      <td>Fri May 22 08:50:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>philwade</td>\n",
       "      <td>Guess there's a first time for everything, my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710275</th>\n",
       "      <td>0</td>\n",
       "      <td>2257860040</td>\n",
       "      <td>Sat Jun 20 15:00:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JBnVFCLover786</td>\n",
       "      <td>My cousin is going to America and it's NOT FAI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367641</th>\n",
       "      <td>0</td>\n",
       "      <td>2049251254</td>\n",
       "      <td>Fri Jun 05 16:33:26 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ChelseyHart</td>\n",
       "      <td>@mitchelmusso Ahh this is my first comment to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575674</th>\n",
       "      <td>0</td>\n",
       "      <td>2211153017</td>\n",
       "      <td>Wed Jun 17 12:30:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>GGirl33</td>\n",
       "      <td>taking care of my good friend jessica she is s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment          id                          time      flag  \\\n",
       "250977          0  1983354434  Sun May 31 12:59:58 PDT 2009  NO_QUERY   \n",
       "150064          0  1883616871  Fri May 22 08:50:14 PDT 2009  NO_QUERY   \n",
       "710275          0  2257860040  Sat Jun 20 15:00:38 PDT 2009  NO_QUERY   \n",
       "367641          0  2049251254  Fri Jun 05 16:33:26 PDT 2009  NO_QUERY   \n",
       "575674          0  2211153017  Wed Jun 17 12:30:03 PDT 2009  NO_QUERY   \n",
       "\n",
       "              username                                               text  \n",
       "250977         Yushimi  @Kardboard yea  ugh. I don't wanna move either...  \n",
       "150064        philwade  Guess there's a first time for everything, my ...  \n",
       "710275  JBnVFCLover786  My cousin is going to America and it's NOT FAI...  \n",
       "367641     ChelseyHart  @mitchelmusso Ahh this is my first comment to ...  \n",
       "575674         GGirl33  taking care of my good friend jessica she is s...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./train.csv\", header=None, encoding='latin-1', sep=',')\n",
    "train.columns = ['sentiment', 'id', 'time', 'flag', 'username', 'text']\n",
    "train = train.iloc[np.random.permutation(len(train))][:100000]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>flag</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  id                          time     flag  username  \\\n",
       "0          4   3  Mon May 11 03:17:40 UTC 2009  kindle2    tpryan   \n",
       "1          4   4  Mon May 11 03:18:03 UTC 2009  kindle2    vcu451   \n",
       "2          4   5  Mon May 11 03:18:54 UTC 2009  kindle2    chadfu   \n",
       "3          4   6  Mon May 11 03:19:04 UTC 2009  kindle2     SIX15   \n",
       "4          4   7  Mon May 11 03:21:41 UTC 2009  kindle2  yamarama   \n",
       "\n",
       "                                                text  \n",
       "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1  Reading my kindle2...  Love it... Lee childs i...  \n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3  @kenburbary You'll love your Kindle2. I've had...  \n",
       "4  @mikefish  Fair enough. But i have the Kindle2...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"./test.csv\", encoding='latin-1', header=None)\n",
    "test.columns = ['sentiment', 'id', 'time', 'flag', 'username', 'text']\n",
    "test = test.drop(test[test.sentiment == 2].index)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet, flags=re.MULTILINE)\n",
    "    tweet = re.sub(r'[_\"\\-;%()|.,+&=*%]', '', tweet)\n",
    "    tweet = re.sub(r'\\.', ' . ', tweet)\n",
    "    tweet = re.sub(r'\\!', ' !', tweet)\n",
    "    tweet = re.sub(r'\\?', ' ?', tweet)\n",
    "    tweet = re.sub(r'\\,', ' ,', tweet)\n",
    "    tweet = re.sub(r':', ' : ', tweet)\n",
    "    tweet = re.sub(r'#', ' # ', tweet)\n",
    "    #tweet = re.sub(r'@', ' @ ', tweet)\n",
    "    tweet = re.sub(r'd .c .', 'd.c.', tweet)\n",
    "    tweet = re.sub(r'u .s .', 'd.c.', tweet)\n",
    "    tweet = re.sub(r' amp ', ' and ', tweet)\n",
    "    tweet = re.sub(r'pm', ' pm ', tweet)\n",
    "    tweet = re.sub(r'news', ' news ', tweet)\n",
    "    tweet = re.sub(r' . . . ', ' ', tweet)\n",
    "    tweet = re.sub(r' .  .  . ', ' ', tweet)\n",
    "    tweet = re.sub(r' ! ! ', ' ! ', tweet)\n",
    "    tweet = re.sub(r'&amp', 'and', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess(text, remove_stopwords=True):\n",
    "    text = clean_tweet(text)\n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        \n",
    "    msg = \" \".join(text)\n",
    "    \n",
    "    for t in text:\n",
    "        if t.startswith('http'):\n",
    "            msg = msg.replace(t, 'URL')\n",
    "        if t.startswith('@'):\n",
    "            msg = msg.replace(t, 'username')\n",
    "    \n",
    "    text = msg\n",
    "    \n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^a-z]\", \" \", text)\n",
    "    text = re.sub(r\"    \", \" \", text) # Remove any extra spaces\n",
    "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**BEFORE PREPROC:** @Kardboard yea  ugh. I don't wanna move either because all my textbooks and noted are all out on the table. Sigh.\n",
      "----\n",
      "**AFTER PREPROC:**  username yea ugh wanna move either textbooks noted table sigh\n"
     ]
    }
   ],
   "source": [
    "print('**BEFORE PREPROC:**', train['text'].values[0])\n",
    "print('-'*4)\n",
    "print('**AFTER PREPROC:** ', preprocess(train['text'].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250977</th>\n",
       "      <td>username yea ugh wanna move either textbooks n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150064</th>\n",
       "      <td>guess there s first time everything cars broke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710275</th>\n",
       "      <td>cousin going america fair know wayyy pppl does...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367641</th>\n",
       "      <td>username ahh first comment youu love xxxand pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  sentiment\n",
       "250977  username yea ugh wanna move either textbooks n...          0\n",
       "150064  guess there s first time everything cars broke...          0\n",
       "710275  cousin going america fair know wayyy pppl does...          0\n",
       "367641  username ahh first comment youu love xxxand pl...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[['text','sentiment']]\n",
    "train['text'] = list(map(preprocess, train['text']))\n",
    "train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>flag</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>username loooooooovvvvvveee kindle dx cool fan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>reading kindle love lee childs good read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>ok first assesment kindle fucking rocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>username love kindle i ve mine months never lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  id                          time     flag username  \\\n",
       "0          4   3  Mon May 11 03:17:40 UTC 2009  kindle2   tpryan   \n",
       "1          4   4  Mon May 11 03:18:03 UTC 2009  kindle2   vcu451   \n",
       "2          4   5  Mon May 11 03:18:54 UTC 2009  kindle2   chadfu   \n",
       "3          4   6  Mon May 11 03:19:04 UTC 2009  kindle2    SIX15   \n",
       "\n",
       "                                                text  \n",
       "0  username loooooooovvvvvveee kindle dx cool fan...  \n",
       "1           reading kindle love lee childs good read  \n",
       "2           ok first assesment kindle fucking rocks   \n",
       "3  username love kindle i ve mine months never lo...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'] = list(map(preprocess, test['text']))\n",
    "test[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_labels = {0:0, 4:1} # 0:0 -- просто для наглядности:)\n",
    "train.sentiment = [translate_labels[t] for t in train.sentiment]\n",
    "test.sentiment = [translate_labels[t] for t in test.sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"preprocessed_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(min_df=40, max_features=6000)\n",
    "X_train = transformer.fit_transform(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = transformer.transform(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.sentiment\n",
    "y_test = test.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04662513, 0.95337487],\n",
       "       [0.05971427, 0.94028573],\n",
       "       [0.18586407, 0.81413593],\n",
       "       [0.72101228, 0.27898772],\n",
       "       [0.51532979, 0.48467021]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8664400571180232"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, test_pred[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11169451029899066039\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train.text)\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61896"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = tokenizer.texts_to_sequences(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHjJJREFUeJzt3XuUHWWZ7/Hvj1wA5ZKENJyYRIIajwJLA8aQJY6HQU5IgjOJZ8EYDkpgcIKeMKMz6hjQI6hkBlyjOHiBwSESRAnxSg6EiRFBhzUCaSSEhMikCdE0iaSxEwhewITn/FFvD+V+d/fefUlXY36ftWrt2k+9VfVU7dr72fVW7W5FBGZmZmUHVJ2AmZkNPS4OZmaWcXEwM7OMi4OZmWVcHMzMLOPiYGZmGRcHM7N9SNLLJC2QNELSWyS9peqcmuHi0AuStkj6raRnJT0p6auSDqk6LzMbuiLiN8DJwJPAtcCvqs2oOfKP4JonaQvw3oj4gaTxwCrgtohYVG1mZmYDy2cOfRQRTwB3AMcDSDpf0kZJuyVtlnRhub2kOZLWSnpG0mOSZqb43ZJ+l85Gnk1nJltK822RdLGkRyTtTGcrB5WmvyMtd5ek/5D0hpr13iTp+dKy20vTDpT0T5J+kc6ErpV0cGn6JElRym2vpPemaQdIWpS25VeSlksaUzPf8Jo8Lkvjp9Tk8Rep/XtLsb9M+3OnpFWSju7p9ZDUXjqre17STTXTy/v5d5LuqZerpGnp+eX1ck2xeySd100el0n6fWmfPZuWNylNP1zSjZI6JP1c0sclHVCa/69Kx9Ejkk5stA1p2vT0+u+S9JCkU2ryuryU169rtvlASZ+XtC0Nn5d0YGn7X0jz7ZZ0v6TjS8v9pqRfSnpa0o8lHZfi76o5bv7rGE/Tuz1+SsuOlOuzKfeu1+S88rbXzNPj65XW+/G073ek1+LwUtu3lvbj1rSuPm9Lo+NrKHNx6CNJE4HZwIMptAN4B3AYcD5wVdcbW9I04EbgI8Ao4G3AltLiLoqIQyLiEODP6qzuHOB04NXAa4GPp+WeCCwBLgSOAP4FWNH1xu5KFViclj2rZrlXpuVNAV4DjAc+UZredXwcnub/99K0vwHmAv8DeAWwE/hSndx7JGkE8Glgeyk2F7gE+F9AS1rvzY0WBcxMef5DnekHAAvT9Pf1sJzPAE80vQH13VJ6PUfVTPsCcDjwKop9dy7F8YKks4DLUuww4M/5wy6Iutug4iz2duByYAzwYeDbklpK8wq4Kc17XE1OHwOmUxwHbwSmkY6xZFtpWx5KOXa5A5gMHAn8FPg6QESU98G/84fHODQ4fkoF8w1pnq8zMM5Lw59SvAaHAF9M63xl2p4vUBx3U4C1/d2WGgNxfA0KF4fe+56kXcA9wI9IH0QRcXtEPBaFHwHfB/4kzXMBsCQiVkfECxHxRET8rBfr/GJEbI2ITmAxcHaK/xXwLxFxX0TsjYilwHMUb/QuBwPP1y5QktL8fxsRnRGxO23LvFKzkcALEbG3Tk4XAh+LiPaIeI7iA+NMlc4WmnQhcB/wnzWxf4yIjRGxJ+U1RT2fPdTdzpKRDaYj6R0U74kfNJN4b0kaBrwLuDgidkfEFuCzwHtSk/cCn4mINek4aouIn5cW0d02vBtYGREr0/G1Gmil+PLSpaf9cw7wqYjYEREdwCdLOZUdAAyjVLAiYknalq5j4I3lb+I9aHT8jEyPPb5mfXAO8LmI2BwRzwIXA/PSes8BfhARN0fE7yPiVxGxtollNvVe2NfH10Bzcei9uRExKiKOjoj/ExG/BZA0S9K9kjpT8ZgNjE3zTAQe68c6t5bGf07x7QTgaOBD6RR4V1rvxNJ0gP8GdNRZZgvwMuCB0rz/luJdxlB8C6rnaOC7pXk3AnuBo0ptnipN/4vaBUg6FPh74P/WWfY/l+btpPjmO75eIulMaVQ329nMtkDxXvjHlE+tV9Ts4+l12jRjLMWHXvkD/+e8uF2NjpPutuFo4KyaHN8KjCu16e44gOJ4qc2pfAy9Ii1zN8XZ5xegKHaSrkjdKc/w4tnwWBprdPx0dTF195pNT/N2pm6gqbX5dvN61dvW4Wm9fX2fNvNe6On4GpJcHAZA+nD6NvBPwFERMQpYSfGBBsWH+6v7sYqJpfFXAttKy12cilXX8LKIuDnlNYLimshDdZb5FPBb4LjSvF3dR11eyx9+oy/bCsyqWfdB6VpMl7Fd04DldZbxEWB5zbfjrmVfWLPsgyPiP7rJZQrFB9fj9SZKGknxBu5uW6Doang0Iu6tM21bORegXptmPAX8PuXS5ZW82M3Q7XHSYBu2Al+r2V8vj4grSm1OoP5xAMXxVJvTtvL0tN0HA4sojnWA/w3MAU6j6Cqb1JVuN+upzbmn4+e1wPb07b6ee1NOLcBqUtdQOd9uXq9627qH4k6ivr5Pm3kvnEf3x9eQ5OIwMEYCB1J8M9sjaRYwozT9euB8SW9PF6/GS3pdL5a/UNKEdJHrEuCWFP8K8D5JJ6nwcklnpG/kUPRl/5Kii+EPRMQLaf6rJB0JRd+1pNPT+ETgA8D3usnpWmBxV1ePpBZJc3qxTYem/BZ3s+yL9eLFzcNTf3wm9U3/NfDNet1fKi7efwJoi4ieisPHKLoY9pmU33KK/XZo2nd/B3RdPP9X4MOS3pRez9dIOrqJbbgJ+DNJp6dv8wepuDA7AUDSDIozhzu6Se1m4OPpNRyb1nVTbaOICOAFXjwzOJSiG/NXFGeh9a71dKfb4yflsIjuj71yTnuBp2n+s+xm4G8lHaPiNvR/oLhGtIfiusZpKm6QGC7pCElT+rMtJfv8+BpoLg4DIPXX/w3FG38nxTeqFaXp95MuUlMcyD/iD7+9NPINimsYm9NweVpuK8V1gy+m9bZRfENB0jkUF6iPAXaruLPiDopT7mvTcj+a5rk3dQv8APjvadoq4O6Ucz3/nLbx+5J2U3w7O6kX23QYcHVEZN0GEfFdiovly1Je68kvpne5lqKv+N168Q6SS4B3pX3wceAtwJkN8rktIjb1Iv+++mvg1xSv4z0Ur+0SgIj4JkWx/AbFmdD3KLpXetyGiNhK8Q3+EoovKFspzsoOkPQnFK/7ocAv0/7ZkGb9f+nxcoovEOuAhykuLJfvpnlF2re70zr+MsVvpOiWeQJ4hN6dUfV0/Cyj+Cbf0y3ib1Zxh1o7xev/gSbXuwT4GvBjijPN31G8JkTELyi6gz9E0ZW5luICfX+2pctgHV8Dxr9zGOJU+m1FL+c7D5gUEZfVxCcAl0fEeQOUYqUk3QDcEBF318TfDQyPiBsqSGvIUHFL63n1Xm9JP4iI0wY9KXtJ6O2dJfbS8WvgmTrxPRTfiv5YdFJ0bdT6NT6+odg33b3ePV3At/2czxyGuL6eOZiZ9YeLg5mZZXxB2szMMi/ZPtmxY8fGpEmTqk7DzOwl5YEHHngqIloatXvJFodJkybR2prdvm9mZj2QVPuj07rcrWRmZhkXBzMzy7g4mJlZxsXBzMwyLg5mZpZxcTAzs4yLg5mZZVwczMws4+JgZmaZl+wvpF+KJi26vbJ1b7nijMrWbWYvPT5zMDOzjIuDmZllXBzMzCzj4mBmZpmGxUHSQZLul/SQpA2SPpniN0h6XNLaNExJcUm6WlKbpHWSTiwta76kTWmYX4q/SdLDaZ6rJWlfbKyZmTWnmbuVngNOjYhnJY0A7pF0R5r2kYj4Vk37WcDkNJwEXAOcJGkMcCkwFQjgAUkrImJnarMAuBdYCcwE7sDMzCrR8MwhCs+mpyPS0NM/np4D3JjmuxcYJWkccDqwOiI6U0FYDcxM0w6LiJ9E8Q+tbwTm9mObzMysn5q65iBpmKS1wA6KD/j70qTFqevoKkkHpth4YGtp9vYU6yneXideL48FkloltXZ0dDSTupmZ9UFTxSEi9kbEFGACME3S8cDFwOuANwNjgI+m5vWuF0Qf4vXyuC4ipkbE1JaWhv8C1czM+qhXdytFxC7gbmBmRGxPXUfPAV8FpqVm7cDE0mwTgG0N4hPqxM3MrCLN3K3UImlUGj8YOA34WbpWQLqzaC6wPs2yAjg33bU0HXg6IrYDq4AZkkZLGg3MAFalabslTU/LOhe4dWA308zMeqOZu5XGAUslDaMoJssj4jZJP5TUQtEttBZ4X2q/EpgNtAG/Ac4HiIhOSZ8G1qR2n4qIzjT+fuAG4GCKu5R8p5KZWYUaFoeIWAecUCd+ajftA1jYzbQlwJI68Vbg+Ea5mJnZ4PAvpM3MLOPiYGZmGRcHMzPLuDiYmVnGxcHMzDIuDmZmlnFxMDOzjIuDmZllXBzMzCzj4mBmZhkXBzMzy7g4mJlZxsXBzMwyLg5mZpZxcTAzs4yLg5mZZVwczMws4+JgZmYZFwczM8s0LA6SDpJ0v6SHJG2Q9MkUP0bSfZI2SbpF0sgUPzA9b0vTJ5WWdXGKPyrp9FJ8Zoq1SVo08JtpZma90cyZw3PAqRHxRmAKMFPSdOBK4KqImAzsBC5I7S8AdkbEa4CrUjskHQvMA44DZgJfljRM0jDgS8As4Fjg7NTWzMwq0rA4ROHZ9HREGgI4FfhWii8F5qbxOek5afrbJSnFl0XEcxHxONAGTEtDW0RsjojngWWprZmZVaSpaw7pG/5aYAewGngM2BURe1KTdmB8Gh8PbAVI058GjijHa+bpLl4vjwWSWiW1dnR0NJO6mZn1QVPFISL2RsQUYALFN/3X12uWHtXNtN7G6+VxXURMjYipLS0tjRM3M7M+6dXdShGxC7gbmA6MkjQ8TZoAbEvj7cBEgDT9cKCzHK+Zp7u4mZlVpJm7lVokjUrjBwOnARuBu4AzU7P5wK1pfEV6Tpr+w4iIFJ+X7mY6BpgM3A+sASanu59GUly0XjEQG2dmZn0zvHETxgFL011FBwDLI+I2SY8AyyRdDjwIXJ/aXw98TVIbxRnDPICI2CBpOfAIsAdYGBF7ASRdBKwChgFLImLDgG2hmZn1WsPiEBHrgBPqxDdTXH+ojf8OOKubZS0GFteJrwRWNpGvmZkNAv9C2szMMi4OZmaWcXEwM7OMi4OZmWVcHMzMLOPiYGZmGRcHMzPLuDiYmVnGxcHMzDIuDmZmlnFxMDOzjIuDmZllXBzMzCzj4mBmZplm/p+D/RGYtOj2Sta75YozKlmvmfWPzxzMzCzj4mBmZhkXBzMzy7g4mJlZpmFxkDRR0l2SNkraIOkDKX6ZpCckrU3D7NI8F0tqk/SopNNL8Zkp1iZpUSl+jKT7JG2SdIukkQO9oWZm1rxmzhz2AB+KiNcD04GFko5N066KiClpWAmQps0DjgNmAl+WNEzSMOBLwCzgWODs0nKuTMuaDOwELhig7TMzsz5oWBwiYntE/DSN7wY2AuN7mGUOsCwinouIx4E2YFoa2iJic0Q8DywD5kgScCrwrTT/UmBuXzfIzMz6r1fXHCRNAk4A7kuhiyStk7RE0ugUGw9sLc3WnmLdxY8AdkXEnpq4mZlVpOniIOkQ4NvAByPiGeAa4NXAFGA78NmupnVmjz7E6+WwQFKrpNaOjo5mUzczs15qqjhIGkFRGL4eEd8BiIgnI2JvRLwAfIWi2wiKb/4TS7NPALb1EH8KGCVpeE08ExHXRcTUiJja0tLSTOpmZtYHzdytJOB6YGNEfK4UH1dq9k5gfRpfAcyTdKCkY4DJwP3AGmByujNpJMVF6xUREcBdwJlp/vnArf3bLDMz649m/rbSycB7gIclrU2xSyjuNppC0QW0BbgQICI2SFoOPEJxp9PCiNgLIOkiYBUwDFgSERvS8j4KLJN0OfAgRTEyM7OKNCwOEXEP9a8LrOxhnsXA4jrxlfXmi4jNvNgtZWZmFfMvpM3MLOPiYGZmGRcHMzPLuDiYmVnGxcHMzDIuDmZmlnFxMDOzjIuDmZllXBzMzCzj4mBmZhkXBzMzy7g4mJlZxsXBzMwyLg5mZpZxcTAzs4yLg5mZZVwczMws4+JgZmYZFwczM8u4OJiZWaZhcZA0UdJdkjZK2iDpAyk+RtJqSZvS4+gUl6SrJbVJWifpxNKy5qf2myTNL8XfJOnhNM/VkrQvNtbMzJrTzJnDHuBDEfF6YDqwUNKxwCLgzoiYDNyZngPMAianYQFwDRTFBLgUOAmYBlzaVVBSmwWl+Wb2f9PMzKyvGhaHiNgeET9N47uBjcB4YA6wNDVbCsxN43OAG6NwLzBK0jjgdGB1RHRGxE5gNTAzTTssIn4SEQHcWFqWmZlVoFfXHCRNAk4A7gOOiojtUBQQ4MjUbDywtTRbe4r1FG+vE6+3/gWSWiW1dnR09CZ1MzPrhaaLg6RDgG8DH4yIZ3pqWicWfYjnwYjrImJqRExtaWlplLKZmfVRU8VB0giKwvD1iPhOCj+ZuoRIjztSvB2YWJp9ArCtQXxCnbiZmVWkmbuVBFwPbIyIz5UmrQC67jiaD9xaip+b7lqaDjydup1WATMkjU4XomcAq9K03ZKmp3WdW1qWmZlVYHgTbU4G3gM8LGltil0CXAEsl3QB8AvgrDRtJTAbaAN+A5wPEBGdkj4NrEntPhURnWn8/cANwMHAHWkwM7OKNCwOEXEP9a8LALy9TvsAFnazrCXAkjrxVuD4RrmYmdng8C+kzcws4+JgZmYZFwczM8u4OJiZWcbFwczMMi4OZmaWcXEwM7OMi4OZmWVcHMzMLOPiYGZmGRcHMzPLuDiYmVnGxcHMzDLN/MnuPzqTFt1edQpmZkOazxzMzCzj4mBmZpn9slvJBk+VXXhbrjijsnWbvdT5zMHMzDIuDmZmlmlYHCQtkbRD0vpS7DJJT0ham4bZpWkXS2qT9Kik00vxmSnWJmlRKX6MpPskbZJ0i6SRA7mBZmbWe82cOdwAzKwTvyoipqRhJYCkY4F5wHFpni9LGiZpGPAlYBZwLHB2agtwZVrWZGAncEF/NsjMzPqvYXGIiB8DnU0ubw6wLCKei4jHgTZgWhraImJzRDwPLAPmSBJwKvCtNP9SYG4vt8HMzAZYf645XCRpXep2Gp1i44GtpTbtKdZd/AhgV0TsqYmbmVmF+locrgFeDUwBtgOfTXHVaRt9iNclaYGkVkmtHR0dvcvYzMya1qfiEBFPRsTeiHgB+ApFtxEU3/wnlppOALb1EH8KGCVpeE28u/VeFxFTI2JqS0tLX1I3M7Mm9Kk4SBpXevpOoOtOphXAPEkHSjoGmAzcD6wBJqc7k0ZSXLReEREB3AWcmeafD9zal5zMzGzgNPyFtKSbgVOAsZLagUuBUyRNoegC2gJcCBARGyQtBx4B9gALI2JvWs5FwCpgGLAkIjakVXwUWCbpcuBB4PoB2zozM+uThsUhIs6uE+72AzwiFgOL68RXAivrxDfzYreUmZkNAf6FtJmZZVwczMws4+JgZmYZFwczM8u4OJiZWcbFwczMMi4OZmaWcXEwM7OMi4OZmWVcHMzMLOPiYGZmGRcHMzPLuDiYmVnGxcHMzDIuDmZmlnFxMDOzjIuDmZllXBzMzCzj4mBmZhkXBzMzyzQsDpKWSNohaX0pNkbSakmb0uPoFJekqyW1SVon6cTSPPNT+02S5pfib5L0cJrnakka6I00M7PeaebM4QZgZk1sEXBnREwG7kzPAWYBk9OwALgGimICXAqcBEwDLu0qKKnNgtJ8tesyM7NB1rA4RMSPgc6a8BxgaRpfCswtxW+Mwr3AKEnjgNOB1RHRGRE7gdXAzDTtsIj4SUQEcGNpWWZmVpG+XnM4KiK2A6THI1N8PLC11K49xXqKt9eJ1yVpgaRWSa0dHR19TN3MzBoZ6AvS9a4XRB/idUXEdRExNSKmtrS09DFFMzNrpK/F4cnUJUR63JHi7cDEUrsJwLYG8Ql14mZmVqG+FocVQNcdR/OBW0vxc9NdS9OBp1O30ypghqTR6UL0DGBVmrZb0vR0l9K5pWWZmVlFhjdqIOlm4BRgrKR2iruOrgCWS7oA+AVwVmq+EpgNtAG/Ac4HiIhOSZ8G1qR2n4qIrovc76e4I+pg4I40mJlZhRoWh4g4u5tJb6/TNoCF3SxnCbCkTrwVOL5RHmZmNnj8C2kzM8u4OJiZWcbFwczMMi4OZmaWcXEwM7OMi4OZmWVcHMzMLOPiYGZmGRcHMzPLuDiYmVnGxcHMzDIuDmZmlnFxMDOzjIuDmZllXBzMzCzj4mBmZhkXBzMzy7g4mJlZxsXBzMwy/SoOkrZIeljSWkmtKTZG0mpJm9Lj6BSXpKsltUlaJ+nE0nLmp/abJM3v3yaZmVl/DcSZw59GxJSImJqeLwLujIjJwJ3pOcAsYHIaFgDXQFFMgEuBk4BpwKVdBcXMzKqxL7qV5gBL0/hSYG4pfmMU7gVGSRoHnA6sjojOiNgJrAZm7oO8zMysSf0tDgF8X9IDkhak2FERsR0gPR6Z4uOBraV521Osu3hG0gJJrZJaOzo6+pm6mZl1Z3g/5z85IrZJOhJYLelnPbRVnVj0EM+DEdcB1wFMnTq1bhszM+u/fp05RMS29LgD+C7FNYMnU3cR6XFHat4OTCzNPgHY1kPczMwq0ufiIOnlkg7tGgdmAOuBFUDXHUfzgVvT+Arg3HTX0nTg6dTttAqYIWl0uhA9I8XMzKwi/elWOgr4rqSu5XwjIv5N0hpguaQLgF8AZ6X2K4HZQBvwG+B8gIjolPRpYE1q96mI6OxHXmZm1k99Lg4RsRl4Y534r4C314kHsLCbZS0BlvQ1FzMzG1j+hbSZmWX6e7eS2ZA1adHtlax3yxVnVLJes4HkMwczM8u4OJiZWcbFwczMMi4OZmaWcXEwM7OMi4OZmWVcHMzMLOPiYGZmGRcHMzPLuDiYmVnGxcHMzDIuDmZmlnFxMDOzjIuDmZllXBzMzCzj4mBmZhkXBzMzy7g4mJlZZsgUB0kzJT0qqU3SoqrzMTPbnw2J/yEtaRjwJeB/Au3AGkkrIuKRajMz672q/nc1+P9X28AZKmcO04C2iNgcEc8Dy4A5FedkZrbfGhJnDsB4YGvpeTtwUm0jSQuABenps5Ie7eP6xgJP9XHefcl59Y7zqqEre5zs/dU7f6x5Hd1Mo6FSHFQnFlkg4jrgun6vTGqNiKn9Xc5Ac16947x6x3n1zv6e11DpVmoHJpaeTwC2VZSLmdl+b6gUhzXAZEnHSBoJzANWVJyTmdl+a0h0K0XEHkkXAauAYcCSiNiwD1fZ766pfcR59Y7z6h3n1Tv7dV6KyLr2zcxsPzdUupXMzGwIcXEwM7PMflUchvKf6JC0RdLDktZKaq0wjyWSdkhaX4qNkbRa0qb0OHqI5HWZpCfSPlsraXYFeU2UdJekjZI2SPpAile6z3rIq9J9JukgSfdLeijl9ckUP0bSfWl/3ZJuTBkKed0g6fHS/poymHmlHIZJelDSben54OyriNgvBooL3Y8BrwJGAg8Bx1adVym/LcDYIZDH24ATgfWl2GeARWl8EXDlEMnrMuDDFe+vccCJafxQ4D+BY6veZz3kVek+o/hN0yFpfARwHzAdWA7MS/FrgfcPkbxuAM6s+Bj7O+AbwG3p+aDsq/3pzMF/oqMJEfFjoLMmPAdYmsaXAnMHNSm6zatyEbE9In6axncDGyl+8V/pPushr0pF4dn0dEQaAjgV+FaKV7G/usurUpImAGcA/5qei0HaV/tTcaj3Jzoqf7OUBPB9SQ+kPxMylBwVEduh+NABjqw4n7KLJK1L3U6D3t1VJmkScALFt84hs89q8oKK91nqJlkL7ABWU5zR74qIPalJJe/N2rwiomt/LU776ypJBw5yWp8H/h54IT0/gkHaV/tTcWjqT3RU6OSIOBGYBSyU9LaqE3oJuAZ4NTAF2A58tqpEJB0CfBv4YEQ8U1UeterkVfk+i4i9ETGF4i8hTANeX6/Z4GaV5yXpeOBi4HXAm4ExwEcHKx9J7wB2RMQD5XCdpvtkX+1PxWFI/4mOiNiWHncA36V40wwVT0oaB5Aed1ScDwAR8WR6Q78AfIWK9pmkERQfwF+PiO+kcOX7rF5eQ2WfpVx2AXdT9O2PktT1o9xK35ulvGam7rmIiOeArzK4++tk4M8lbaHoBj+V4kxiUPbV/lQchuyf6JD0ckmHdo0DM4D1Pc81qFYA89P4fODWCnP5L10fvsk7qWCfpT7g64GNEfG50qRK91l3eVW9zyS1SBqVxg8GTqO4HnIXcGZqVsX+qpfXz0oFXhR9+4O2vyLi4oiYEBGTKD6vfhgR5zBY+6rKq/CDPQCzKe7aeAz4WNX5lPJ6FcXdUw8BG6rMDbiZorvh9xRnWxdQ9HPeCWxKj2OGSF5fAx4G1lF8GI+rIK+3UpzWrwPWpmF21fush7wq3WfAG4AH0/rXA59I8VcB9wNtwDeBA4dIXj9M+2s9cBPpjqYKjrNTePFupUHZV/7zGWZmltmfupXMzKxJLg5mZpZxcTAzs4yLg5mZZVwczMws4+JgZmYZFwczM8v8f4NJZfBEJ1VrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(x) for x in train_seqs])\n",
    "plt.title(\"Распределение длин последовательностей\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(train_seqs, maxlen=20, dtype='int32', padding='post', truncating='pre', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 20)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_glove = 'glove.twitter.27B.100d.txt'\n",
    "glove_vocab = []\n",
    "glove_embd=[]\n",
    "embedding_dict = {}\n",
    " \n",
    "file = open(filepath_glove,'r',encoding='UTF-8')\n",
    "for line in file.readlines():\n",
    "    row = line.strip().split(' ')\n",
    "    vocab_word = row[0]\n",
    "    glove_vocab.append(vocab_word)\n",
    "    embed_vector = [float(i) for i in row[1:]] # convert to list of float\n",
    "    embedding_dict[vocab_word]=embed_vector\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61896"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prepare embedding matrix\n",
    "num_words = vocab_size+1\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61897, 100)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.5651e-02, -1.4665e-02, -2.0531e-01, -1.3928e-01, -4.5531e-01,\n",
       "        6.6880e-01, -1.5448e-01,  3.2308e-01, -5.1561e-01,  9.6171e-02,\n",
       "       -3.6832e-02,  2.7032e-01, -2.8622e+00,  4.1572e-01, -2.2041e-01,\n",
       "       -8.2353e-01, -7.0891e-01, -3.1757e-01, -7.4595e-01,  1.1605e+00,\n",
       "       -2.9937e-04, -1.3276e-02,  5.3844e-01,  3.2003e-01,  3.9048e-01,\n",
       "       -2.2494e+00, -6.9214e-01, -3.2294e-01,  6.8427e-01,  5.9943e-01,\n",
       "       -9.1371e-02, -5.6070e-01, -3.1685e-01, -1.1708e-01,  1.7076e+00,\n",
       "       -6.3555e-01, -4.3287e-01, -2.2535e-01,  3.5899e-01,  2.3817e-01,\n",
       "       -1.8517e+00,  3.0939e-01,  4.7932e-01, -8.7741e-01, -1.8584e-01,\n",
       "       -5.0600e-01,  2.8379e-02, -7.2791e-01,  8.4749e-02, -7.0856e-01,\n",
       "        3.5898e-01, -2.6869e-01,  1.2857e-01,  3.7286e-01,  3.6686e-01,\n",
       "       -2.9323e-01,  3.7081e-01, -6.0298e-01,  2.4040e-01, -5.0550e-01,\n",
       "       -2.8867e-01, -4.5598e-01,  7.7508e-02,  1.0154e-01,  2.9443e-01,\n",
       "        4.2504e-01, -2.3515e-01,  4.4486e-01, -1.5291e-01, -1.0447e-01,\n",
       "        2.9614e-01, -6.3704e-01,  3.1613e-01,  2.7299e-01, -6.2677e-01,\n",
       "        5.8366e-01, -7.3846e-01,  1.0140e+00, -7.8185e-02, -5.2589e-01,\n",
       "        1.4557e+00,  3.6629e-01,  3.7527e-01,  3.3924e-02, -1.3459e-01,\n",
       "        7.2309e-01, -7.9871e-01,  1.2158e-01,  2.5512e-01,  5.7559e-01,\n",
       "        5.3372e-01, -3.7430e-01, -2.1475e-01,  1.2555e-01,  7.2887e-01,\n",
       "       -5.7377e-01, -8.9212e-01, -3.1556e-01,  4.4199e-01, -4.0687e-01])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_size_analisys(BATCH_SIZE):   \n",
    "    emb_size = 100\n",
    "    BATCH_SIZE = 100\n",
    "    learning_rate = 0.001\n",
    "    EPOCHS = 1\n",
    "    y_train = np.array(train.sentiment)\n",
    "    y_train = np.expand_dims(y_train, axis=1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((padded, y_train)).shuffle(BATCH_SIZE).repeat().batch(BATCH_SIZE)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    get_next = iterator.get_next()\n",
    "    print(get_next)\n",
    "    inputs, labels = get_next\n",
    "    with tf.variable_scope( \"embedding_mtx\", reuse=tf.AUTO_REUSE):\n",
    "        embedding_mtx = tf.get_variable(name=\"embedding_mtx\",\n",
    "                                        shape=embedding_matrix.shape,\n",
    "                                        initializer=tf.constant_initializer(embedding_matrix),\n",
    "                                        trainable=False)\n",
    "    inputs_embedded = tf.nn.embedding_lookup(params=embedding_mtx, ids=inputs)\n",
    "    with tf.variable_scope( \"states\", reuse=tf.AUTO_REUSE):\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell=tf.nn.rnn_cell.LSTMCell(100), inputs=inputs_embedded, dtype=tf.float32)\n",
    "    outputs = tf.layers.flatten(outputs)\n",
    "    logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "    probs = tf.nn.sigmoid(logits)\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=tf.cast(labels, dtype=tf.float32),\n",
    "        logits=logits,\n",
    "        name='loss'\n",
    "    )\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    with tf.variable_scope( \"train_op\", reuse=tf.AUTO_REUSE):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    losses = []\n",
    "    num_epochs = 1\n",
    "    num_iter_per_epoch = 3000\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_epochs):\n",
    "        print('Epoch ',i)\n",
    "        aver_loss = 0.\n",
    "        for j in range(num_iter_per_epoch):\n",
    "            loss_cur, _ = sess.run([loss, train_op])\n",
    "            aver_loss += loss_cur\n",
    "            if j % 300 == 0:\n",
    "                print('Loss: ', aver_loss / 300.)\n",
    "                aver_loss = 0.\n",
    "\n",
    "            losses.append(loss_cur)\n",
    "\n",
    "    test_seqs = tokenizer.texts_to_sequences(test.text)\n",
    "    padded_test = pad_sequences(test_seqs, maxlen=20, dtype='int32', padding='post', truncating='pre', value=0.0)\n",
    "    test_pred = sess.run(probs, feed_dict={inputs: padded_test})\n",
    "    print('Roc auc score: ', roc_auc_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext:0' shape=(?, 20) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(?, 1) dtype=int64>)\n",
      "Epoch  0\n",
      "Loss:  0.002310922940572103\n",
      "Loss:  0.5431242530544599\n",
      "Loss:  0.5129935674866041\n",
      "Loss:  0.5009064045548439\n",
      "Loss:  0.4928040125966072\n",
      "Loss:  0.48247292002042136\n",
      "Loss:  0.48241712162892025\n",
      "Loss:  0.47761032929023106\n",
      "Loss:  0.46694743474324546\n",
      "Loss:  0.46837378581364947\n",
      "Roc auc score:  0.8859191655801825\n"
     ]
    }
   ],
   "source": [
    "batch_size_analisys(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext_1:0' shape=(?, 20) dtype=int32>, <tf.Tensor 'IteratorGetNext_1:1' shape=(?, 1) dtype=int64>)\n",
      "Epoch  0\n",
      "Loss:  0.002321139176686605\n",
      "Loss:  0.5405810878674189\n",
      "Loss:  0.5144708662231763\n",
      "Loss:  0.5009268839160601\n",
      "Loss:  0.4932903834184011\n",
      "Loss:  0.48203336834907534\n",
      "Loss:  0.4826710312565168\n",
      "Loss:  0.47875728805859885\n",
      "Loss:  0.46718964775403343\n",
      "Loss:  0.46742642352978386\n",
      "Roc auc score:  0.8797106847954306\n"
     ]
    }
   ],
   "source": [
    "batch_size_analisys(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext_2:0' shape=(?, 20) dtype=int32>, <tf.Tensor 'IteratorGetNext_2:1' shape=(?, 1) dtype=int64>)\n",
      "Epoch  0\n",
      "Loss:  0.0022823023796081543\n",
      "Loss:  0.5455331586798032\n",
      "Loss:  0.5139759284257889\n",
      "Loss:  0.5010217472910881\n",
      "Loss:  0.49125659515460335\n",
      "Loss:  0.480667317310969\n",
      "Loss:  0.48108950098355613\n",
      "Loss:  0.47586575845877327\n",
      "Loss:  0.46419204662243524\n",
      "Loss:  0.46649639040231705\n",
      "Roc auc score:  0.8796175575836591\n"
     ]
    }
   ],
   "source": [
    "batch_size_analisys(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext_3:0' shape=(?, 20) dtype=int32>, <tf.Tensor 'IteratorGetNext_3:1' shape=(?, 1) dtype=int64>)\n",
      "Epoch  0\n",
      "Loss:  0.0023321032524108885\n",
      "Loss:  0.5448206051190694\n",
      "Loss:  0.5145217421650886\n",
      "Loss:  0.5016254949569702\n",
      "Loss:  0.4936995334426562\n",
      "Loss:  0.4832457122206688\n",
      "Loss:  0.4827340105175972\n",
      "Loss:  0.47842856516440707\n",
      "Loss:  0.46603852609793345\n",
      "Loss:  0.4672432368000348\n",
      "Roc auc score:  0.8842739181722232\n"
     ]
    }
   ],
   "source": [
    "batch_size_analisys(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я вывел значение roc auc при батчах размера 200, 150, 100 и 50. Можем заметить, что с уменьшением батча значение roc auc увеличивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_lstm_analisys(num_cells):   \n",
    "\n",
    "    emb_size = 100\n",
    "    BATCH_SIZE = 100\n",
    "    learning_rate = 0.001\n",
    "    EPOCHS = 1\n",
    "    y_train = np.array(train.sentiment)\n",
    "    y_train = np.expand_dims(y_train, axis=1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((padded, y_train)).shuffle(BATCH_SIZE).repeat().batch(BATCH_SIZE)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    get_next = iterator.get_next()\n",
    "    print(get_next)\n",
    "    inputs, labels = get_next\n",
    "    with tf.variable_scope( \"embedding_mtx\", reuse=tf.AUTO_REUSE):\n",
    "        embedding_mtx = tf.get_variable(name=\"embedding_mtx\",\n",
    "                                        shape=embedding_matrix.shape,\n",
    "                                        initializer=tf.constant_initializer(embedding_matrix),\n",
    "                                        trainable=False)\n",
    "    inputs_embedded = tf.nn.embedding_lookup(params=embedding_mtx, ids=inputs)\n",
    "    \n",
    "    stacked_rnn = []\n",
    "    for iiLyr in range(num_cells):\n",
    "        stacked_rnn.append(tf.nn.rnn_cell.LSTMCell(100, state_is_tuple=True))\n",
    "    MultiLyr_cell = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn, state_is_tuple=True)\n",
    "    \n",
    "    with tf.variable_scope( \"states\", reuse=tf.AUTO_REUSE):\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell=MultiLyr_cell, inputs=inputs_embedded, dtype=tf.float32)\n",
    "    outputs = tf.layers.flatten(outputs)\n",
    "    logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "    probs = tf.nn.sigmoid(logits)\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=tf.cast(labels, dtype=tf.float32),\n",
    "        logits=logits,\n",
    "        name='loss'\n",
    "    )\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    with tf.variable_scope( \"train_op\", reuse=tf.AUTO_REUSE):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    losses = []\n",
    "    num_epochs = 1\n",
    "    num_iter_per_epoch = 3000\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_epochs):\n",
    "        print('Epoch ',i)\n",
    "        aver_loss = 0.\n",
    "        for j in range(num_iter_per_epoch):\n",
    "            loss_cur, _ = sess.run([loss, train_op])\n",
    "            aver_loss += loss_cur\n",
    "            if j % 300 == 0:\n",
    "                print('Loss: ', aver_loss / 200.)\n",
    "                aver_loss = 0.\n",
    "\n",
    "            losses.append(loss_cur)\n",
    "\n",
    "    test_seqs = tokenizer.texts_to_sequences(test.text)\n",
    "    padded_test = pad_sequences(test_seqs, maxlen=20, dtype='int32', padding='post', truncating='pre', value=0.0)\n",
    "    test_pred = sess.run(probs, feed_dict={inputs: padded_test})\n",
    "    print('Roc auc score: ', roc_auc_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext_4:0' shape=(?, 20) dtype=int32>, <tf.Tensor 'IteratorGetNext_4:1' shape=(?, 1) dtype=int64>)\n",
      "Epoch  0\n",
      "Loss:  0.003492025136947632\n",
      "Loss:  0.812435449808836\n",
      "Loss:  0.7725379377603531\n",
      "Loss:  0.7491498173773289\n",
      "Loss:  0.738094082325697\n",
      "Loss:  0.7245496365427971\n",
      "Loss:  0.7241321703791619\n",
      "Loss:  0.7171112543344498\n",
      "Loss:  0.7013353668153286\n",
      "Loss:  0.7042051681876182\n",
      "Roc auc score:  0.8900788477059663\n"
     ]
    }
   ],
   "source": [
    "multiple_lstm_analisys(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext_5:0' shape=(?, 20) dtype=int32>, <tf.Tensor 'IteratorGetNext_5:1' shape=(?, 1) dtype=int64>)\n",
      "Epoch  0\n",
      "Loss:  0.0034739920496940613\n",
      "Loss:  0.8136301659047603\n",
      "Loss:  0.7748345257341862\n",
      "Loss:  0.7533482572436333\n",
      "Loss:  0.7410206462442875\n",
      "Loss:  0.7283304740488529\n",
      "Loss:  0.7249749009311199\n",
      "Loss:  0.7181708399951457\n",
      "Loss:  0.7026716002821922\n",
      "Loss:  0.7047063168883324\n",
      "Roc auc score:  0.8932141305022661\n"
     ]
    }
   ],
   "source": [
    "multiple_lstm_analisys(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext_6:0' shape=(?, 20) dtype=int32>, <tf.Tensor 'IteratorGetNext_6:1' shape=(?, 1) dtype=int64>)\n",
      "Epoch  0\n",
      "Loss:  0.0034765979647636415\n",
      "Loss:  0.8220951153337955\n",
      "Loss:  0.7753786681592465\n",
      "Loss:  0.7560034257173538\n",
      "Loss:  0.7427360098063945\n",
      "Loss:  0.7267723239958286\n",
      "Loss:  0.7272418949007988\n",
      "Loss:  0.7206654554605484\n",
      "Loss:  0.7042898027598858\n",
      "Loss:  0.7044924329221248\n",
      "Roc auc score:  0.8868193952939715\n"
     ]
    }
   ],
   "source": [
    "multiple_lstm_analisys(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я взял 1, 2, 3 и 4 слоя LSTM, можно заметить, что значение roc auc увеличивается с увеличением слоёв. Но, если мы возьмём 4 слоя, то значение станет ухудшаться. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_output_analisys():   \n",
    "    emb_size = 100\n",
    "    BATCH_SIZE = 100\n",
    "    learning_rate = 0.001\n",
    "    EPOCHS = 1\n",
    "    y_train = np.array(train.sentiment)\n",
    "    y_train = np.expand_dims(y_train, axis=1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((padded, y_train)).shuffle(BATCH_SIZE).repeat().batch(BATCH_SIZE)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    get_next = iterator.get_next()\n",
    "    print(get_next)\n",
    "    inputs, labels = get_next\n",
    "    with tf.variable_scope( \"embedding_mtx\", reuse=tf.AUTO_REUSE):\n",
    "        embedding_mtx = tf.get_variable(name=\"embedding_mtx\",\n",
    "                                        shape=embedding_matrix.shape,\n",
    "                                        initializer=tf.constant_initializer(embedding_matrix),\n",
    "                                        trainable=False)\n",
    "    inputs_embedded = tf.nn.embedding_lookup(params=embedding_mtx, ids=inputs)\n",
    "    with tf.variable_scope( \"states\", reuse=tf.AUTO_REUSE):\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell=tf.nn.rnn_cell.LSTMCell(100), inputs=inputs_embedded, dtype=tf.float32)\n",
    "    # states.h return the last output or the last hidden state\n",
    "    outputs = tf.layers.flatten(states.h)\n",
    "    logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "    probs = tf.nn.sigmoid(logits)\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=tf.cast(labels, dtype=tf.float32),\n",
    "        logits=logits,\n",
    "        name='loss'\n",
    "    )\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    with tf.variable_scope( \"train_op\", reuse=tf.AUTO_REUSE):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    losses = []\n",
    "    num_epochs = 1\n",
    "    num_iter_per_epoch = 3000\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_epochs):\n",
    "        print('Epoch ',i)\n",
    "        aver_loss = 0.\n",
    "        for j in range(num_iter_per_epoch):\n",
    "            loss_cur, _ = sess.run([loss, train_op])\n",
    "            aver_loss += loss_cur\n",
    "            if j % 300 == 0:\n",
    "                print('Loss: ', aver_loss / 300.)\n",
    "                aver_loss = 0.\n",
    "\n",
    "            losses.append(loss_cur)\n",
    "\n",
    "    test_seqs = tokenizer.texts_to_sequences(test.text)\n",
    "    padded_test = pad_sequences(test_seqs, maxlen=20, dtype='int32', padding='post', truncating='pre', value=0.0)\n",
    "    test_pred = sess.run(probs, feed_dict={inputs: padded_test})\n",
    "    print('Roc auc score: ', roc_auc_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext_7:0' shape=(?, 20) dtype=int32>, <tf.Tensor 'IteratorGetNext_7:1' shape=(?, 1) dtype=int64>)\n",
      "Epoch  0\n",
      "Loss:  0.0023188279072443646\n",
      "Loss:  0.5459324980775515\n",
      "Loss:  0.5195074926813443\n",
      "Loss:  0.5043394421537717\n",
      "Loss:  0.4949584251642227\n",
      "Loss:  0.4865829625725746\n",
      "Loss:  0.485296055773894\n",
      "Loss:  0.48038550128539403\n",
      "Loss:  0.4710685606797536\n",
      "Loss:  0.4719792298475901\n",
      "Roc auc score:  0.8877196250077606\n"
     ]
    }
   ],
   "source": [
    "last_output_analisys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если взять только последние выходы на последнем шаге, то значение roc_auc увеличится. Было  0.8796175575836591, стало 0.8877196250077606."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
